{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1\n",
    "\n",
    "Points: 20\n",
    "\n",
    "You will be working with RGBNet: a network that accepts pixel position as input and outputs a triplet with R, G, B channels of that pixels.\n",
    "RGBNet is trained on a fixed image. Your tasks are:\n",
    "\n",
    "1. (14 points) Fill gaps in the code, which creates embeddings in 2 ways:\n",
    "    - Learned embedding of size 64 (7 points)\n",
    "    - Positional embedding of size 64 (7 points)\n",
    "\n",
    "\n",
    "Please note that your code should train within 1 minute and report training loss below 15 for each case.\n",
    "2. (6 points) Visualize output of the network for each encoding. Does it resemble the input image?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import urllib\n",
    "from typing import Literal\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "IMG_URL = \"https://i.natgeofe.com/k/8fa25ea4-6409-47fb-b3cc-4af8e0dc9616/red-eyed-tree-frog-on-leaves-3-2.jpg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "url_response = urllib.request.urlopen(IMG_URL)\n",
    "img = cv2.imdecode(np.array(bytearray(url_response.read()), dtype=np.uint8), -1)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "img = cv2.resize(img, (0,0), fx=0.01, fy=0.01) \n",
    "im_w, im_h = img.shape[0], img.shape[1]\n",
    "\n",
    "plt.imshow(img)\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveEncoding(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(2, 64)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return self.linear(x.float())\n",
    "\n",
    "\n",
    "class LearnedEncoding(nn.Module):\n",
    "    def __init__(self) -> None:\n",
    "        super().__init__()\n",
    "        self.x_embedding = nn.Embedding(im_w, 32)\n",
    "        self.y_embedding = nn.Embedding(im_h, 32)       \n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.cat((self.x_embedding(x[:, 0]), self.y_embedding(x[:, 1])), dim=-1)\n",
    "\n",
    "\n",
    "class PositionalEncoding(nn.Module):    \n",
    "  def __init__(self) -> None:\n",
    "    super().__init__()  \n",
    "    self.d_model = 64\n",
    "    self.div_term = torch.exp(torch.arange(0, self.d_model // 2, 2).float()*(-torch.log(torch.Tensor([10.0])) / self.d_model // 2)).T.unsqueeze(0) \n",
    "\n",
    "  def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "    pe_x = torch.zeros(x.size(0), self.d_model // 2)\n",
    "    pe_x[:, 0::2] = torch.sin(x[:, 0].unsqueeze(1) * self.div_term)        \n",
    "    pe_x[:, 1::2] = torch.cos(x[:, 0].unsqueeze(1) * self.div_term) \n",
    "\n",
    "    pe_y = torch.zeros(x.size(0), self.d_model // 2)\n",
    "    pe_y[:, 0::2] = torch.sin(x[:, 1].unsqueeze(1) * self.div_term)        \n",
    "    pe_y[:, 1::2] = torch.cos(x[:, 1].unsqueeze(1) * self.div_term)   \n",
    "\n",
    "    return torch.cat([pe_x, pe_y], dim=1)  \n",
    "\n",
    "\n",
    "# Define the network\n",
    "class RGBNet(nn.Module):\n",
    "    def __init__(self, encoding_type: Literal[\"naive\", \"learned\", \"positional\"]) -> None:\n",
    "        super().__init__()\n",
    "        if encoding_type == \"naive\":\n",
    "            self.encoding = NaiveEncoding()\n",
    "        elif encoding_type == \"learned\":\n",
    "            self.encoding = LearnedEncoding() \n",
    "        elif encoding_type == \"positional\":\n",
    "            self.encoding = PositionalEncoding()\n",
    "        else:\n",
    "            raise ValueError(\"Wrong encoding type!\")\n",
    "        self.fc1 = nn.Linear(64, 128)\n",
    "        self.fc2 = nn.Linear(128, 128)\n",
    "        self.fc3 = nn.Linear(128, 3)\n",
    "        \n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = self.encoding(x)\n",
    "        x = F.softplus(self.fc1(x))\n",
    "        x = F.softplus(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(used_embedding: Literal[\"naive\", \"learned\", \"positional\"]) -> torch.nn.Module:\n",
    "    # Instantiate the model and set it to the GPU (if available)\n",
    "    model = RGBNet(encoding_type=used_embedding)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Define the loss function and optimizer\n",
    "    criterion = nn.MSELoss(reduction=\"mean\")\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "\n",
    "    # Define the number of epochs and batch size\n",
    "    num_epochs = 300\n",
    "    batch_size = 32\n",
    "\n",
    "    X, y = torch.cartesian_prod(torch.tensor(range(im_w)), torch.tensor(range(im_h))).to(device), torch.flatten(torch.tensor(img, dtype=torch.float32), start_dim=0, end_dim=1).to(device)\n",
    "\n",
    "    model = model.train()\n",
    "\n",
    "    # Train the model\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0\n",
    "        perm = torch.randperm(X.size(0))\n",
    "        X, y = X[perm,:], y[perm, :]\n",
    "        for i in range(0, X.shape[0], batch_size):\n",
    "            # Get the current batch\n",
    "            X_batch = X[i:i+batch_size]\n",
    "            y_batch = y[i:i+batch_size]\n",
    "            \n",
    "            # Zero the gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs, y_batch)\n",
    "            \n",
    "            # Backward pass and optimization\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss/X.size(0)}')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_output(model: RGBNet) -> None:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    X = torch.cartesian_prod(torch.tensor(range(im_w)), torch.tensor(range(im_h))).to(device)\n",
    "    model.eval()\n",
    "    y_pred = model(X)\n",
    "\n",
    "    y_pred = y_pred.view(im_w,im_h,3).long().detach().cpu().numpy()\n",
    "\n",
    "    plt.imshow(y_pred)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 142.93318661456834\n",
      "Epoch [2/300], Loss: 99.95835489721343\n",
      "Epoch [3/300], Loss: 81.27373817549324\n",
      "Epoch [4/300], Loss: 69.41179899576073\n",
      "Epoch [5/300], Loss: 74.96248292703233\n",
      "Epoch [6/300], Loss: 65.99191206830987\n",
      "Epoch [7/300], Loss: 63.60057426487795\n",
      "Epoch [8/300], Loss: 57.113132951446396\n",
      "Epoch [9/300], Loss: 63.738692780244186\n",
      "Epoch [10/300], Loss: 54.33149958316082\n",
      "Epoch [11/300], Loss: 55.5446283015238\n",
      "Epoch [12/300], Loss: 49.937124893961965\n",
      "Epoch [13/300], Loss: 55.77938772456437\n",
      "Epoch [14/300], Loss: 54.834808701194376\n",
      "Epoch [15/300], Loss: 53.71642113610896\n",
      "Epoch [16/300], Loss: 49.626993838543164\n",
      "Epoch [17/300], Loss: 50.36072290657852\n",
      "Epoch [18/300], Loss: 55.30360982165359\n",
      "Epoch [19/300], Loss: 54.78060125535534\n",
      "Epoch [20/300], Loss: 55.1770513859762\n",
      "Epoch [21/300], Loss: 50.515020695699526\n",
      "Epoch [22/300], Loss: 46.71161335184827\n",
      "Epoch [23/300], Loss: 45.50724395189417\n",
      "Epoch [24/300], Loss: 44.17769272975658\n",
      "Epoch [25/300], Loss: 51.498519405241936\n",
      "Epoch [26/300], Loss: 50.57298306935394\n",
      "Epoch [27/300], Loss: 65.13098260554301\n",
      "Epoch [28/300], Loss: 46.757491502893686\n",
      "Epoch [29/300], Loss: 53.86541073003672\n",
      "Epoch [30/300], Loss: 60.185330017371115\n",
      "Epoch [31/300], Loss: 44.945289576657906\n",
      "Epoch [32/300], Loss: 46.02250516689318\n",
      "Epoch [33/300], Loss: 48.99244675878006\n",
      "Epoch [34/300], Loss: 54.13860851938274\n",
      "Epoch [35/300], Loss: 44.62044486472134\n",
      "Epoch [36/300], Loss: 46.07077873687041\n",
      "Epoch [37/300], Loss: 44.624563999439715\n",
      "Epoch [38/300], Loss: 43.85215502620293\n",
      "Epoch [39/300], Loss: 40.13567387453422\n",
      "Epoch [40/300], Loss: 40.79717602707823\n",
      "Epoch [41/300], Loss: 42.500977617255\n",
      "Epoch [42/300], Loss: 50.35711416780674\n",
      "Epoch [43/300], Loss: 45.33673812936528\n",
      "Epoch [44/300], Loss: 45.13099128969254\n",
      "Epoch [45/300], Loss: 51.46920670891687\n",
      "Epoch [46/300], Loss: 41.864970404980916\n",
      "Epoch [47/300], Loss: 37.845593817223055\n",
      "Epoch [48/300], Loss: 40.6642807639689\n",
      "Epoch [49/300], Loss: 42.5316399780836\n",
      "Epoch [50/300], Loss: 46.44252583727859\n",
      "Epoch [51/300], Loss: 39.66283676920948\n",
      "Epoch [52/300], Loss: 38.143153950915355\n",
      "Epoch [53/300], Loss: 38.965628065821214\n",
      "Epoch [54/300], Loss: 42.052315777897284\n",
      "Epoch [55/300], Loss: 39.64746413692351\n",
      "Epoch [56/300], Loss: 41.87783018894459\n",
      "Epoch [57/300], Loss: 41.561712168328775\n",
      "Epoch [58/300], Loss: 39.925725910520775\n",
      "Epoch [59/300], Loss: 37.364206995282856\n",
      "Epoch [60/300], Loss: 40.119942590388284\n",
      "Epoch [61/300], Loss: 41.09391345186717\n",
      "Epoch [62/300], Loss: 39.54425667617728\n",
      "Epoch [63/300], Loss: 38.8979748844551\n",
      "Epoch [64/300], Loss: 38.768351682320166\n",
      "Epoch [65/300], Loss: 38.545023553382414\n",
      "Epoch [66/300], Loss: 37.8385263609996\n",
      "Epoch [67/300], Loss: 38.91280551330285\n",
      "Epoch [68/300], Loss: 35.98951686032906\n",
      "Epoch [69/300], Loss: 37.0648189140355\n",
      "Epoch [70/300], Loss: 42.07811339769495\n",
      "Epoch [71/300], Loss: 42.223423056888144\n",
      "Epoch [72/300], Loss: 49.72788298294841\n",
      "Epoch [73/300], Loss: 42.954354281799034\n",
      "Epoch [74/300], Loss: 54.46745138563868\n",
      "Epoch [75/300], Loss: 42.975799630863875\n",
      "Epoch [76/300], Loss: 38.443551621678786\n",
      "Epoch [77/300], Loss: 43.31622722291727\n",
      "Epoch [78/300], Loss: 38.89586699173747\n",
      "Epoch [79/300], Loss: 44.82767236946915\n",
      "Epoch [80/300], Loss: 43.58858606013285\n",
      "Epoch [81/300], Loss: 37.83134214339718\n",
      "Epoch [82/300], Loss: 39.77579951835668\n",
      "Epoch [83/300], Loss: 41.02616896387619\n",
      "Epoch [84/300], Loss: 42.34836666265391\n",
      "Epoch [85/300], Loss: 42.48135847100465\n",
      "Epoch [86/300], Loss: 41.742196359942035\n",
      "Epoch [87/300], Loss: 39.138275005850375\n",
      "Epoch [88/300], Loss: 39.166836189234864\n",
      "Epoch [89/300], Loss: 37.16390871672037\n",
      "Epoch [90/300], Loss: 34.974312162619036\n",
      "Epoch [91/300], Loss: 41.60735825569399\n",
      "Epoch [92/300], Loss: 36.23952158914733\n",
      "Epoch [93/300], Loss: 36.0912595124838\n",
      "Epoch [94/300], Loss: 35.397830506074264\n",
      "Epoch [95/300], Loss: 38.1598879554854\n",
      "Epoch [96/300], Loss: 33.52223137007331\n",
      "Epoch [97/300], Loss: 36.86436560846144\n",
      "Epoch [98/300], Loss: 35.99489990796911\n",
      "Epoch [99/300], Loss: 35.85568657448764\n",
      "Epoch [100/300], Loss: 34.33066762757191\n",
      "Epoch [101/300], Loss: 34.721986480572255\n",
      "Epoch [102/300], Loss: 39.58731993222566\n",
      "Epoch [103/300], Loss: 46.22768954088062\n",
      "Epoch [104/300], Loss: 46.10926783689156\n",
      "Epoch [105/300], Loss: 39.19172679004581\n",
      "Epoch [106/300], Loss: 35.21101931382984\n",
      "Epoch [107/300], Loss: 39.012880316527756\n",
      "Epoch [108/300], Loss: 36.47266679755004\n",
      "Epoch [109/300], Loss: 33.95286215505293\n",
      "Epoch [110/300], Loss: 39.045393666913434\n",
      "Epoch [111/300], Loss: 39.62388470205843\n",
      "Epoch [112/300], Loss: 38.21844475390175\n",
      "Epoch [113/300], Loss: 38.281398509504605\n",
      "Epoch [114/300], Loss: 38.28519320158365\n",
      "Epoch [115/300], Loss: 39.57418148198985\n",
      "Epoch [116/300], Loss: 38.058739376507596\n",
      "Epoch [117/300], Loss: 34.36600568217616\n",
      "Epoch [118/300], Loss: 36.94853284282069\n",
      "Epoch [119/300], Loss: 39.5059683487712\n",
      "Epoch [120/300], Loss: 35.01733053884199\n",
      "Epoch [121/300], Loss: 44.97329662692162\n",
      "Epoch [122/300], Loss: 36.22549487698463\n",
      "Epoch [123/300], Loss: 34.80687066706644\n",
      "Epoch [124/300], Loss: 32.87146495344452\n",
      "Epoch [125/300], Loss: 40.956512275379374\n",
      "Epoch [126/300], Loss: 34.108361064014346\n",
      "Epoch [127/300], Loss: 36.52844463295651\n",
      "Epoch [128/300], Loss: 35.79314137823571\n",
      "Epoch [129/300], Loss: 35.29132375409526\n",
      "Epoch [130/300], Loss: 35.44233991912982\n",
      "Epoch [131/300], Loss: 37.49171465333156\n",
      "Epoch [132/300], Loss: 33.745542675668744\n",
      "Epoch [133/300], Loss: 31.48819999870617\n",
      "Epoch [134/300], Loss: 32.50061425415602\n",
      "Epoch [135/300], Loss: 35.70385292158698\n",
      "Epoch [136/300], Loss: 34.94856937250234\n",
      "Epoch [137/300], Loss: 32.81408919936501\n",
      "Epoch [138/300], Loss: 37.31993939799647\n",
      "Epoch [139/300], Loss: 31.364903893888272\n",
      "Epoch [140/300], Loss: 37.58752666420651\n",
      "Epoch [141/300], Loss: 33.43931372154693\n",
      "Epoch [142/300], Loss: 34.29774369622156\n",
      "Epoch [143/300], Loss: 33.13867556664251\n",
      "Epoch [144/300], Loss: 43.35905442479569\n",
      "Epoch [145/300], Loss: 32.953607444938974\n",
      "Epoch [146/300], Loss: 33.85877526630454\n",
      "Epoch [147/300], Loss: 32.71697505827873\n",
      "Epoch [148/300], Loss: 29.914339232554635\n",
      "Epoch [149/300], Loss: 33.752317894438995\n",
      "Epoch [150/300], Loss: 31.165149759037703\n",
      "Epoch [151/300], Loss: 30.510248122676725\n",
      "Epoch [152/300], Loss: 32.54031256047262\n",
      "Epoch [153/300], Loss: 31.300656068160237\n",
      "Epoch [154/300], Loss: 35.68367218861382\n",
      "Epoch [155/300], Loss: 33.36540756577171\n",
      "Epoch [156/300], Loss: 35.84785064134729\n",
      "Epoch [157/300], Loss: 38.59298804498488\n",
      "Epoch [158/300], Loss: 36.879902782528085\n",
      "Epoch [159/300], Loss: 30.400286678894325\n",
      "Epoch [160/300], Loss: 29.618418117822042\n",
      "Epoch [161/300], Loss: 30.283738905383693\n",
      "Epoch [162/300], Loss: 29.598997458884245\n",
      "Epoch [163/300], Loss: 31.639236590829313\n",
      "Epoch [164/300], Loss: 35.7807441307103\n",
      "Epoch [165/300], Loss: 36.6897485574819\n",
      "Epoch [166/300], Loss: 36.479864094114525\n",
      "Epoch [167/300], Loss: 33.994336668796805\n",
      "Epoch [168/300], Loss: 31.535377432124406\n",
      "Epoch [169/300], Loss: 30.82620418566163\n",
      "Epoch [170/300], Loss: 39.043060443368375\n",
      "Epoch [171/300], Loss: 34.65240823068926\n",
      "Epoch [172/300], Loss: 35.90018308437365\n",
      "Epoch [173/300], Loss: 35.668336894655006\n",
      "Epoch [174/300], Loss: 38.23606179936141\n",
      "Epoch [175/300], Loss: 35.23790888632497\n",
      "Epoch [176/300], Loss: 31.524285417547972\n",
      "Epoch [177/300], Loss: 32.196995396767896\n",
      "Epoch [178/300], Loss: 36.38429794663109\n",
      "Epoch [179/300], Loss: 40.87539504090762\n",
      "Epoch [180/300], Loss: 32.69516054496238\n",
      "Epoch [181/300], Loss: 33.21483696440947\n",
      "Epoch [182/300], Loss: 28.89285464660363\n",
      "Epoch [183/300], Loss: 28.532236090453537\n",
      "Epoch [184/300], Loss: 29.755720305552682\n",
      "Epoch [185/300], Loss: 29.289493402577765\n",
      "Epoch [186/300], Loss: 30.624026109546012\n",
      "Epoch [187/300], Loss: 31.710004182455176\n",
      "Epoch [188/300], Loss: 30.89029820174116\n",
      "Epoch [189/300], Loss: 35.667133032451574\n",
      "Epoch [190/300], Loss: 37.96125385618429\n",
      "Epoch [191/300], Loss: 31.93071959535098\n",
      "Epoch [192/300], Loss: 29.489726097353042\n",
      "Epoch [193/300], Loss: 29.474268478182605\n",
      "Epoch [194/300], Loss: 35.049989128991754\n",
      "Epoch [195/300], Loss: 34.42982873169508\n",
      "Epoch [196/300], Loss: 43.38030761507799\n",
      "Epoch [197/300], Loss: 33.58814095352103\n",
      "Epoch [198/300], Loss: 37.138691880186585\n",
      "Epoch [199/300], Loss: 35.0958494546776\n",
      "Epoch [200/300], Loss: 34.29824568928662\n",
      "Epoch [201/300], Loss: 45.93822742936798\n",
      "Epoch [202/300], Loss: 40.567789701822164\n",
      "Epoch [203/300], Loss: 35.75342830534904\n",
      "Epoch [204/300], Loss: 31.215049585439093\n",
      "Epoch [205/300], Loss: 33.099600761167466\n",
      "Epoch [206/300], Loss: 32.44160586238457\n",
      "Epoch [207/300], Loss: 29.1485109812653\n",
      "Epoch [208/300], Loss: 31.19957933557748\n",
      "Epoch [209/300], Loss: 35.87053430794571\n",
      "Epoch [210/300], Loss: 33.66668954313076\n",
      "Epoch [211/300], Loss: 35.3103287165066\n",
      "Epoch [212/300], Loss: 39.43234372468588\n",
      "Epoch [213/300], Loss: 38.36263507192585\n",
      "Epoch [214/300], Loss: 31.36135029243434\n",
      "Epoch [215/300], Loss: 31.194559158817416\n",
      "Epoch [216/300], Loss: 37.57808899769585\n",
      "Epoch [217/300], Loss: 30.952264108965473\n",
      "Epoch [218/300], Loss: 32.64266436880085\n",
      "Epoch [219/300], Loss: 34.54845839821249\n",
      "Epoch [220/300], Loss: 32.70708208919121\n",
      "Epoch [221/300], Loss: 32.80516839796497\n",
      "Epoch [222/300], Loss: 30.846657968336537\n",
      "Epoch [223/300], Loss: 29.73523195328251\n",
      "Epoch [224/300], Loss: 30.43172866188436\n",
      "Epoch [225/300], Loss: 29.52401399392686\n",
      "Epoch [226/300], Loss: 29.548521147345618\n",
      "Epoch [227/300], Loss: 30.93692340059764\n",
      "Epoch [228/300], Loss: 29.037774292554722\n",
      "Epoch [229/300], Loss: 30.909929530411823\n",
      "Epoch [230/300], Loss: 31.040588572278\n",
      "Epoch [231/300], Loss: 27.133400279805407\n",
      "Epoch [232/300], Loss: 29.3111701121528\n",
      "Epoch [233/300], Loss: 30.86754850747948\n",
      "Epoch [234/300], Loss: 29.524576565087667\n",
      "Epoch [235/300], Loss: 29.747119973881453\n",
      "Epoch [236/300], Loss: 28.591590248494654\n",
      "Epoch [237/300], Loss: 27.889299975012854\n",
      "Epoch [238/300], Loss: 28.517092515796012\n",
      "Epoch [239/300], Loss: 27.39749282625963\n",
      "Epoch [240/300], Loss: 28.77364936512187\n",
      "Epoch [241/300], Loss: 34.004410915111066\n",
      "Epoch [242/300], Loss: 28.463110822686403\n",
      "Epoch [243/300], Loss: 40.106856456000685\n",
      "Epoch [244/300], Loss: 44.96628647905341\n",
      "Epoch [245/300], Loss: 28.741805995114937\n",
      "Epoch [246/300], Loss: 27.253997662100375\n",
      "Epoch [247/300], Loss: 32.710508706932245\n",
      "Epoch [248/300], Loss: 31.89920624060565\n",
      "Epoch [249/300], Loss: 29.877032794161327\n",
      "Epoch [250/300], Loss: 30.94103233935097\n",
      "Epoch [251/300], Loss: 32.51066552439043\n",
      "Epoch [252/300], Loss: 30.214008243402578\n",
      "Epoch [253/300], Loss: 28.949611224337108\n",
      "Epoch [254/300], Loss: 30.023841646959156\n",
      "Epoch [255/300], Loss: 27.290314511769378\n",
      "Epoch [256/300], Loss: 28.084172086232268\n",
      "Epoch [257/300], Loss: 30.46938774003411\n",
      "Epoch [258/300], Loss: 36.304414441508634\n",
      "Epoch [259/300], Loss: 30.20863947011359\n",
      "Epoch [260/300], Loss: 29.73718950825353\n",
      "Epoch [261/300], Loss: 28.891467995357953\n",
      "Epoch [262/300], Loss: 31.798351375738047\n",
      "Epoch [263/300], Loss: 28.372514083088817\n",
      "Epoch [264/300], Loss: 26.403306635843443\n",
      "Epoch [265/300], Loss: 29.081880736460885\n",
      "Epoch [266/300], Loss: 29.48175261537051\n",
      "Epoch [267/300], Loss: 31.85178484235491\n",
      "Epoch [268/300], Loss: 37.940930696127054\n",
      "Epoch [269/300], Loss: 32.63610439036848\n",
      "Epoch [270/300], Loss: 27.979202973677815\n",
      "Epoch [271/300], Loss: 33.14788041356522\n",
      "Epoch [272/300], Loss: 32.03142965993574\n",
      "Epoch [273/300], Loss: 41.01164780014671\n",
      "Epoch [274/300], Loss: 31.28281005296839\n",
      "Epoch [275/300], Loss: 35.502652075982866\n",
      "Epoch [276/300], Loss: 29.716927242718533\n",
      "Epoch [277/300], Loss: 29.698394740232125\n",
      "Epoch [278/300], Loss: 30.120745118312573\n",
      "Epoch [279/300], Loss: 27.707948570427256\n",
      "Epoch [280/300], Loss: 28.648338177237093\n",
      "Epoch [281/300], Loss: 32.76240427043581\n",
      "Epoch [282/300], Loss: 31.9903808453116\n",
      "Epoch [283/300], Loss: 27.936138223393172\n",
      "Epoch [284/300], Loss: 27.743723829770417\n",
      "Epoch [285/300], Loss: 28.25967080252511\n",
      "Epoch [286/300], Loss: 26.866351342970326\n",
      "Epoch [287/300], Loss: 29.13953600061654\n",
      "Epoch [288/300], Loss: 30.003926817722583\n",
      "Epoch [289/300], Loss: 38.711243941487254\n",
      "Epoch [290/300], Loss: 31.435043299802437\n",
      "Epoch [291/300], Loss: 32.461276076356384\n",
      "Epoch [292/300], Loss: 34.22845163652974\n",
      "Epoch [293/300], Loss: 30.55077154625396\n",
      "Epoch [294/300], Loss: 27.932254105669013\n",
      "Epoch [295/300], Loss: 27.521005410752537\n",
      "Epoch [296/300], Loss: 26.282835999941497\n",
      "Epoch [297/300], Loss: 29.49859068910098\n",
      "Epoch [298/300], Loss: 30.22869837888375\n",
      "Epoch [299/300], Loss: 31.002149344589302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [300/300], Loss: 26.13787615683771\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IMPORTANT: \n",
    "# training code works only for \n",
    "# used_embedding = \"naive\"\n",
    "# training and visualization code should work in both\n",
    "# used_embedding = \"learned\"\n",
    "# used_embedding = \"positonal\"\n",
    "used_embedding = \"naive\"\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = train(used_embedding=used_embedding)\n",
    "visualize_model_output(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 128.57513624621976\n",
      "Epoch [2/300], Loss: 56.93106567804715\n",
      "Epoch [3/300], Loss: 51.762643137285785\n",
      "Epoch [4/300], Loss: 51.36426491451703\n",
      "Epoch [5/300], Loss: 49.62429957236013\n",
      "Epoch [6/300], Loss: 45.679303375806676\n",
      "Epoch [7/300], Loss: 44.7821051250405\n",
      "Epoch [8/300], Loss: 45.73059349235851\n",
      "Epoch [9/300], Loss: 46.17950425389725\n",
      "Epoch [10/300], Loss: 39.474357552242715\n",
      "Epoch [11/300], Loss: 38.90324176841068\n",
      "Epoch [12/300], Loss: 35.17904736918788\n",
      "Epoch [13/300], Loss: 32.46293390511368\n",
      "Epoch [14/300], Loss: 33.04381937475248\n",
      "Epoch [15/300], Loss: 32.725799982448876\n",
      "Epoch [16/300], Loss: 31.21590138800133\n",
      "Epoch [17/300], Loss: 29.46442423877628\n",
      "Epoch [18/300], Loss: 26.982118562619256\n",
      "Epoch [19/300], Loss: 26.445810098252537\n",
      "Epoch [20/300], Loss: 26.890959884713872\n",
      "Epoch [21/300], Loss: 25.139160085933\n",
      "Epoch [22/300], Loss: 25.593888313539566\n",
      "Epoch [23/300], Loss: 25.96682609171362\n",
      "Epoch [24/300], Loss: 23.9325925413914\n",
      "Epoch [25/300], Loss: 22.695484319590204\n",
      "Epoch [26/300], Loss: 20.61794184319984\n",
      "Epoch [27/300], Loss: 19.817967885100895\n",
      "Epoch [28/300], Loss: 19.415792245469333\n",
      "Epoch [29/300], Loss: 19.74148960377214\n",
      "Epoch [30/300], Loss: 19.091500840428786\n",
      "Epoch [31/300], Loss: 17.414474452146187\n",
      "Epoch [32/300], Loss: 17.237894295547417\n",
      "Epoch [33/300], Loss: 15.748957409836729\n",
      "Epoch [34/300], Loss: 14.481480998377647\n",
      "Epoch [35/300], Loss: 15.021977385068269\n",
      "Epoch [36/300], Loss: 13.353024003692486\n",
      "Epoch [37/300], Loss: 11.84137961392029\n",
      "Epoch [38/300], Loss: 12.24243649249802\n",
      "Epoch [39/300], Loss: 13.111019380630985\n",
      "Epoch [40/300], Loss: 10.610256986134612\n",
      "Epoch [41/300], Loss: 11.929161186042469\n",
      "Epoch [42/300], Loss: 9.71542692404189\n",
      "Epoch [43/300], Loss: 9.022173815608573\n",
      "Epoch [44/300], Loss: 9.232405596614432\n",
      "Epoch [45/300], Loss: 10.158541718935638\n",
      "Epoch [46/300], Loss: 10.07708005421722\n",
      "Epoch [47/300], Loss: 7.6078460271457375\n",
      "Epoch [48/300], Loss: 6.8604124570222496\n",
      "Epoch [49/300], Loss: 6.631858808104344\n",
      "Epoch [50/300], Loss: 6.186185397310741\n",
      "Epoch [51/300], Loss: 5.5711691280663835\n",
      "Epoch [52/300], Loss: 5.228375869961927\n",
      "Epoch [53/300], Loss: 4.489899209018128\n",
      "Epoch [54/300], Loss: 4.483805274084417\n",
      "Epoch [55/300], Loss: 4.695847691478817\n",
      "Epoch [56/300], Loss: 4.746954474031651\n",
      "Epoch [57/300], Loss: 3.879056666853241\n",
      "Epoch [58/300], Loss: 3.232476252015285\n",
      "Epoch [59/300], Loss: 2.968450550659461\n",
      "Epoch [60/300], Loss: 3.0233623267318794\n",
      "Epoch [61/300], Loss: 3.504512562729796\n",
      "Epoch [62/300], Loss: 3.8244402661301575\n",
      "Epoch [63/300], Loss: 2.7874414514286725\n",
      "Epoch [64/300], Loss: 2.432722856372183\n",
      "Epoch [65/300], Loss: 3.296832554900701\n",
      "Epoch [66/300], Loss: 2.3822426642141035\n",
      "Epoch [67/300], Loss: 2.142651538145707\n",
      "Epoch [68/300], Loss: 1.9828389163390832\n",
      "Epoch [69/300], Loss: 1.9038417965585734\n",
      "Epoch [70/300], Loss: 1.7860333161419988\n",
      "Epoch [71/300], Loss: 1.6715313432403425\n",
      "Epoch [72/300], Loss: 1.906159584423364\n",
      "Epoch [73/300], Loss: 1.5579116728998\n",
      "Epoch [74/300], Loss: 1.3957850350762293\n",
      "Epoch [75/300], Loss: 1.302227089481969\n",
      "Epoch [76/300], Loss: 1.2485768630207958\n",
      "Epoch [77/300], Loss: 1.397156363808065\n",
      "Epoch [78/300], Loss: 1.3878224632157707\n",
      "Epoch [79/300], Loss: 1.4988724910718505\n",
      "Epoch [80/300], Loss: 1.2738593228951027\n",
      "Epoch [81/300], Loss: 1.377783650077433\n",
      "Epoch [82/300], Loss: 1.283558239035892\n",
      "Epoch [83/300], Loss: 1.332312610292215\n",
      "Epoch [84/300], Loss: 1.1424016985475742\n",
      "Epoch [85/300], Loss: 0.964235901283229\n",
      "Epoch [86/300], Loss: 1.3392561393948743\n",
      "Epoch [87/300], Loss: 1.6560053188130603\n",
      "Epoch [88/300], Loss: 1.40158020845756\n",
      "Epoch [89/300], Loss: 1.9297730516178817\n",
      "Epoch [90/300], Loss: 1.7166502574621807\n",
      "Epoch [91/300], Loss: 1.568073340824672\n",
      "Epoch [92/300], Loss: 1.4554104826966738\n",
      "Epoch [93/300], Loss: 1.2600130452538416\n",
      "Epoch [94/300], Loss: 1.3493192492542179\n",
      "Epoch [95/300], Loss: 3.7679364802101243\n",
      "Epoch [96/300], Loss: 2.6349084212483347\n",
      "Epoch [97/300], Loss: 1.9055912703413018\n",
      "Epoch [98/300], Loss: 1.8154812518352736\n",
      "Epoch [99/300], Loss: 1.4617137733143046\n",
      "Epoch [100/300], Loss: 1.1899298654723278\n",
      "Epoch [101/300], Loss: 1.0483729344908543\n",
      "Epoch [102/300], Loss: 1.5033257589911535\n",
      "Epoch [103/300], Loss: 1.3712165718254405\n",
      "Epoch [104/300], Loss: 0.9965498645184776\n",
      "Epoch [105/300], Loss: 0.8786946683435396\n",
      "Epoch [106/300], Loss: 0.9243598467743341\n",
      "Epoch [107/300], Loss: 0.9305702739047564\n",
      "Epoch [108/300], Loss: 0.72598283631461\n",
      "Epoch [109/300], Loss: 0.6256015223841513\n",
      "Epoch [110/300], Loss: 0.5354275791326426\n",
      "Epoch [111/300], Loss: 0.5930959222503521\n",
      "Epoch [112/300], Loss: 0.631686344674106\n",
      "Epoch [113/300], Loss: 0.7435972855387745\n",
      "Epoch [114/300], Loss: 0.8260887668978784\n",
      "Epoch [115/300], Loss: 1.0551980233961535\n",
      "Epoch [116/300], Loss: 1.2055836734683831\n",
      "Epoch [117/300], Loss: 1.2402797624262796\n",
      "Epoch [118/300], Loss: 1.508280486005792\n",
      "Epoch [119/300], Loss: 1.6237225840168614\n",
      "Epoch [120/300], Loss: 2.0536635570262436\n",
      "Epoch [121/300], Loss: 1.5006864433464366\n",
      "Epoch [122/300], Loss: 1.3686467491536647\n",
      "Epoch [123/300], Loss: 1.3554725624998594\n",
      "Epoch [124/300], Loss: 1.269228372705697\n",
      "Epoch [125/300], Loss: 1.0275565907702469\n",
      "Epoch [126/300], Loss: 0.8426962382233087\n",
      "Epoch [127/300], Loss: 0.7482064550373412\n",
      "Epoch [128/300], Loss: 0.6982506810245426\n",
      "Epoch [129/300], Loss: 0.7124104873375958\n",
      "Epoch [130/300], Loss: 0.8393825726575016\n",
      "Epoch [131/300], Loss: 1.011936588770783\n",
      "Epoch [132/300], Loss: 0.9275027661828951\n",
      "Epoch [133/300], Loss: 1.0475814968759563\n",
      "Epoch [134/300], Loss: 1.465803284798899\n",
      "Epoch [135/300], Loss: 2.328680543855588\n",
      "Epoch [136/300], Loss: 3.4254700994711316\n",
      "Epoch [137/300], Loss: 3.1427129930065525\n",
      "Epoch [138/300], Loss: 2.586246543216266\n",
      "Epoch [139/300], Loss: 2.615960874865132\n",
      "Epoch [140/300], Loss: 1.9287911604077035\n",
      "Epoch [141/300], Loss: 1.5913259994049775\n",
      "Epoch [142/300], Loss: 1.502737385886056\n",
      "Epoch [143/300], Loss: 1.795145432520572\n",
      "Epoch [144/300], Loss: 1.1794910189193515\n",
      "Epoch [145/300], Loss: 1.0154877412154377\n",
      "Epoch [146/300], Loss: 0.9243679112553047\n",
      "Epoch [147/300], Loss: 0.8485325151874173\n",
      "Epoch [148/300], Loss: 1.215728616934218\n",
      "Epoch [149/300], Loss: 1.0784226298881565\n",
      "Epoch [150/300], Loss: 1.7606809369979366\n",
      "Epoch [151/300], Loss: 2.639600731810117\n",
      "Epoch [152/300], Loss: 2.536251107668547\n",
      "Epoch [153/300], Loss: 1.7521974238382507\n",
      "Epoch [154/300], Loss: 2.775916912588656\n",
      "Epoch [155/300], Loss: 1.7730209113266062\n",
      "Epoch [156/300], Loss: 1.3326268064261582\n",
      "Epoch [157/300], Loss: 1.5845768396755517\n",
      "Epoch [158/300], Loss: 1.334277867172171\n",
      "Epoch [159/300], Loss: 1.1068531862601707\n",
      "Epoch [160/300], Loss: 1.051860547834827\n",
      "Epoch [161/300], Loss: 0.8049243814934234\n",
      "Epoch [162/300], Loss: 0.8785006714306669\n",
      "Epoch [163/300], Loss: 0.8768809868992749\n",
      "Epoch [164/300], Loss: 0.8993596503262147\n",
      "Epoch [165/300], Loss: 0.8400319644383022\n",
      "Epoch [166/300], Loss: 0.8503601770796534\n",
      "Epoch [167/300], Loss: 1.1723875241345525\n",
      "Epoch [168/300], Loss: 1.2063802650996618\n",
      "Epoch [169/300], Loss: 1.0289732058476742\n",
      "Epoch [170/300], Loss: 1.6545974812749344\n",
      "Epoch [171/300], Loss: 1.4167239237490887\n",
      "Epoch [172/300], Loss: 1.3680600219058552\n",
      "Epoch [173/300], Loss: 0.9584119330902803\n",
      "Epoch [174/300], Loss: 0.926709367382911\n",
      "Epoch [175/300], Loss: 1.5554039071781844\n",
      "Epoch [176/300], Loss: 1.8224336013266569\n",
      "Epoch [177/300], Loss: 1.2874639748428274\n",
      "Epoch [178/300], Loss: 1.192349146038706\n",
      "Epoch [179/300], Loss: 1.3729077978617585\n",
      "Epoch [180/300], Loss: 1.2971415794390138\n",
      "Epoch [181/300], Loss: 1.352636077986335\n",
      "Epoch [182/300], Loss: 1.8259927160728913\n",
      "Epoch [183/300], Loss: 1.4729617531947825\n",
      "Epoch [184/300], Loss: 1.1734839276784026\n",
      "Epoch [185/300], Loss: 1.3021492738328222\n",
      "Epoch [186/300], Loss: 1.1347105843680245\n",
      "Epoch [187/300], Loss: 1.8863216259512483\n",
      "Epoch [188/300], Loss: 1.4617452973044962\n",
      "Epoch [189/300], Loss: 3.689095127967096\n",
      "Epoch [190/300], Loss: 3.026606638860043\n",
      "Epoch [191/300], Loss: 2.1052577528536047\n",
      "Epoch [192/300], Loss: 1.915056701080041\n",
      "Epoch [193/300], Loss: 1.7735530022651917\n",
      "Epoch [194/300], Loss: 1.8186174010351506\n",
      "Epoch [195/300], Loss: 1.793746214308497\n",
      "Epoch [196/300], Loss: 1.5314507121864003\n",
      "Epoch [197/300], Loss: 0.9786026323995283\n",
      "Epoch [198/300], Loss: 0.886953150621757\n",
      "Epoch [199/300], Loss: 1.1595174140095161\n",
      "Epoch [200/300], Loss: 1.1534159666931574\n",
      "Epoch [201/300], Loss: 0.9262456827998711\n",
      "Epoch [202/300], Loss: 1.495136040146999\n",
      "Epoch [203/300], Loss: 1.185998547461725\n",
      "Epoch [204/300], Loss: 1.1872866714055637\n",
      "Epoch [205/300], Loss: 0.9854737141165316\n",
      "Epoch [206/300], Loss: 1.374231079207038\n",
      "Epoch [207/300], Loss: 1.441638614724858\n",
      "Epoch [208/300], Loss: 1.1795557215466477\n",
      "Epoch [209/300], Loss: 1.0023666658709127\n",
      "Epoch [210/300], Loss: 0.640066223759805\n",
      "Epoch [211/300], Loss: 0.6817522719159105\n",
      "Epoch [212/300], Loss: 0.8948345568872267\n",
      "Epoch [213/300], Loss: 0.7929383324038598\n",
      "Epoch [214/300], Loss: 0.9370711135424776\n",
      "Epoch [215/300], Loss: 0.8132262889141312\n",
      "Epoch [216/300], Loss: 1.533841941763179\n",
      "Epoch [217/300], Loss: 1.2947712889464769\n",
      "Epoch [218/300], Loss: 1.1167309284210205\n",
      "Epoch [219/300], Loss: 0.7738682641411707\n",
      "Epoch [220/300], Loss: 0.7723097801208496\n",
      "Epoch [221/300], Loss: 0.7565465852412211\n",
      "Epoch [222/300], Loss: 0.942724291630055\n",
      "Epoch [223/300], Loss: 1.5102187609342936\n",
      "Epoch [224/300], Loss: 1.4737935132145332\n",
      "Epoch [225/300], Loss: 2.0651497840881348\n",
      "Epoch [226/300], Loss: 2.1553221583915745\n",
      "Epoch [227/300], Loss: 2.5354126543493316\n",
      "Epoch [228/300], Loss: 2.7671719674141175\n",
      "Epoch [229/300], Loss: 1.5442684037344796\n",
      "Epoch [230/300], Loss: 1.7919471252898467\n",
      "Epoch [231/300], Loss: 1.1626239624990296\n",
      "Epoch [232/300], Loss: 0.8358568343149352\n",
      "Epoch [233/300], Loss: 0.6359649067100841\n",
      "Epoch [234/300], Loss: 0.9281244168083789\n",
      "Epoch [235/300], Loss: 0.7513498820467479\n",
      "Epoch [236/300], Loss: 0.7164066859654018\n",
      "Epoch [237/300], Loss: 0.8691576669842417\n",
      "Epoch [238/300], Loss: 0.6540349677960444\n",
      "Epoch [239/300], Loss: 0.49456516224118424\n",
      "Epoch [240/300], Loss: 0.44243900512220674\n",
      "Epoch [241/300], Loss: 0.531101560812392\n",
      "Epoch [242/300], Loss: 0.5329361939759848\n",
      "Epoch [243/300], Loss: 0.8146113832974764\n",
      "Epoch [244/300], Loss: 0.8786278726867817\n",
      "Epoch [245/300], Loss: 0.8904405618043539\n",
      "Epoch [246/300], Loss: 1.3281489539256293\n",
      "Epoch [247/300], Loss: 1.9085692216723746\n",
      "Epoch [248/300], Loss: 2.2401046555163124\n",
      "Epoch [249/300], Loss: 1.5732159350874237\n",
      "Epoch [250/300], Loss: 2.928323829228977\n",
      "Epoch [251/300], Loss: 3.216828768154443\n",
      "Epoch [252/300], Loss: 3.346940229565317\n",
      "Epoch [253/300], Loss: 5.671221772646574\n",
      "Epoch [254/300], Loss: 5.289049083736086\n",
      "Epoch [255/300], Loss: 2.44120878887616\n",
      "Epoch [256/300], Loss: 1.706042689661826\n",
      "Epoch [257/300], Loss: 1.4792074168332712\n",
      "Epoch [258/300], Loss: 0.98405288111779\n",
      "Epoch [259/300], Loss: 0.6646106232146514\n",
      "Epoch [260/300], Loss: 0.46657259618082353\n",
      "Epoch [261/300], Loss: 0.3711728709084647\n",
      "Epoch [262/300], Loss: 0.46162974559766357\n",
      "Epoch [263/300], Loss: 0.34388179383519607\n",
      "Epoch [264/300], Loss: 0.35680485980301957\n",
      "Epoch [265/300], Loss: 0.3785770510748235\n",
      "Epoch [266/300], Loss: 0.2967088524647023\n",
      "Epoch [267/300], Loss: 0.23946167247086625\n",
      "Epoch [268/300], Loss: 0.23980093276995118\n",
      "Epoch [269/300], Loss: 0.23737100192478724\n",
      "Epoch [270/300], Loss: 0.1890274418114517\n",
      "Epoch [271/300], Loss: 0.14903931999536155\n",
      "Epoch [272/300], Loss: 0.15424382192198582\n",
      "Epoch [273/300], Loss: 0.18570258979973156\n",
      "Epoch [274/300], Loss: 0.21397130280595772\n",
      "Epoch [275/300], Loss: 0.21561100076420517\n",
      "Epoch [276/300], Loss: 0.23211997774888843\n",
      "Epoch [277/300], Loss: 0.21771056668549638\n",
      "Epoch [278/300], Loss: 0.26674087585941436\n",
      "Epoch [279/300], Loss: 0.2812847819196464\n",
      "Epoch [280/300], Loss: 0.35290707421192924\n",
      "Epoch [281/300], Loss: 0.3743270047798684\n",
      "Epoch [282/300], Loss: 0.449455793552135\n",
      "Epoch [283/300], Loss: 1.1261859665268577\n",
      "Epoch [284/300], Loss: 1.7421597577459802\n",
      "Epoch [285/300], Loss: 1.6422825870426019\n",
      "Epoch [286/300], Loss: 1.9144281044533724\n",
      "Epoch [287/300], Loss: 2.26417541064425\n",
      "Epoch [288/300], Loss: 4.503698485238211\n",
      "Epoch [289/300], Loss: 3.4878934693226618\n",
      "Epoch [290/300], Loss: 4.497254402406754\n",
      "Epoch [291/300], Loss: 4.179439408438546\n",
      "Epoch [292/300], Loss: 3.3568089568669897\n",
      "Epoch [293/300], Loss: 1.7528137611354002\n",
      "Epoch [294/300], Loss: 0.9701611501280614\n",
      "Epoch [295/300], Loss: 0.7224958865873276\n",
      "Epoch [296/300], Loss: 0.5449159661745695\n",
      "Epoch [297/300], Loss: 0.5068621206942792\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [298/300], Loss: 0.37870219171321884\n",
      "Epoch [299/300], Loss: 0.2984850912050168\n",
      "Epoch [300/300], Loss: 0.38709867165385303\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IMPORTANT: \n",
    "# training code works only for \n",
    "# used_embedding = \"naive\"\n",
    "# training and visualization code should work in both\n",
    "# used_embedding = \"learned\"\n",
    "# used_embedding = \"positonal\"\n",
    "used_embedding = \"learned\"\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = train(used_embedding=used_embedding)\n",
    "visualize_model_output(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/300], Loss: 118.95607043850806\n",
      "Epoch [2/300], Loss: 64.65174577422955\n",
      "Epoch [3/300], Loss: 58.76308096608808\n",
      "Epoch [4/300], Loss: 60.52880437472998\n",
      "Epoch [5/300], Loss: 58.10756482946159\n",
      "Epoch [6/300], Loss: 58.624851490495395\n",
      "Epoch [7/300], Loss: 55.00174108408563\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/zj/t76t6h4x71jcw3vjd48wt79m0000gn/T/ipykernel_13478/2671893781.py:24: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/aten/src/ATen/native/TensorShape.cpp:3281.)\n",
      "  self.div_term = torch.exp(torch.arange(0, self.d_model // 2, 2).float()*(-torch.log(torch.Tensor([10.0])) / self.d_model // 2)).T.unsqueeze(0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8/300], Loss: 55.1867425452729\n",
      "Epoch [9/300], Loss: 55.700071835847496\n",
      "Epoch [10/300], Loss: 57.832040320893036\n",
      "Epoch [11/300], Loss: 55.06233215332031\n",
      "Epoch [12/300], Loss: 56.32163758871193\n",
      "Epoch [13/300], Loss: 55.678759385913196\n",
      "Epoch [14/300], Loss: 55.53399644139725\n",
      "Epoch [15/300], Loss: 57.343226911834854\n",
      "Epoch [16/300], Loss: 58.80640316449003\n",
      "Epoch [17/300], Loss: 60.24464803247408\n",
      "Epoch [18/300], Loss: 56.1438965687554\n",
      "Epoch [19/300], Loss: 54.783463086950064\n",
      "Epoch [20/300], Loss: 54.83139931111841\n",
      "Epoch [21/300], Loss: 54.94806393390427\n",
      "Epoch [22/300], Loss: 53.546568593671246\n",
      "Epoch [23/300], Loss: 53.92962667579475\n",
      "Epoch [24/300], Loss: 53.02059261480235\n",
      "Epoch [25/300], Loss: 51.929666633430166\n",
      "Epoch [26/300], Loss: 50.32144734606765\n",
      "Epoch [27/300], Loss: 51.36791781236499\n",
      "Epoch [28/300], Loss: 53.43727083689606\n",
      "Epoch [29/300], Loss: 50.41474838960006\n",
      "Epoch [30/300], Loss: 49.92771581131193\n",
      "Epoch [31/300], Loss: 56.72337931971396\n",
      "Epoch [32/300], Loss: 56.788506521057975\n",
      "Epoch [33/300], Loss: 51.95812545284148\n",
      "Epoch [34/300], Loss: 51.97965778403568\n",
      "Epoch [35/300], Loss: 48.47848498436712\n",
      "Epoch [36/300], Loss: 50.102246508620304\n",
      "Epoch [37/300], Loss: 51.07376562725015\n",
      "Epoch [38/300], Loss: 52.26910660563526\n",
      "Epoch [39/300], Loss: 61.39937489593084\n",
      "Epoch [40/300], Loss: 49.065413338797434\n",
      "Epoch [41/300], Loss: 50.39396322927168\n",
      "Epoch [42/300], Loss: 49.042392783450644\n",
      "Epoch [43/300], Loss: 47.51655775500882\n",
      "Epoch [44/300], Loss: 49.88908083867368\n",
      "Epoch [45/300], Loss: 51.978912634783626\n",
      "Epoch [46/300], Loss: 46.02599948232624\n",
      "Epoch [47/300], Loss: 45.73533936250045\n",
      "Epoch [48/300], Loss: 52.199586437594505\n",
      "Epoch [49/300], Loss: 50.40995190879716\n",
      "Epoch [50/300], Loss: 46.316745248258385\n",
      "Epoch [51/300], Loss: 44.445576470019084\n",
      "Epoch [52/300], Loss: 44.22042604086037\n",
      "Epoch [53/300], Loss: 48.23627067601076\n",
      "Epoch [54/300], Loss: 45.814473446613086\n",
      "Epoch [55/300], Loss: 45.300435220041585\n",
      "Epoch [56/300], Loss: 43.27727455807172\n",
      "Epoch [57/300], Loss: 46.34831420617169\n",
      "Epoch [58/300], Loss: 45.592419883622554\n",
      "Epoch [59/300], Loss: 46.17723452660345\n",
      "Epoch [60/300], Loss: 45.87441636889761\n",
      "Epoch [61/300], Loss: 48.35896877868934\n",
      "Epoch [62/300], Loss: 43.88238219511674\n",
      "Epoch [63/300], Loss: 41.779858637515304\n",
      "Epoch [64/300], Loss: 40.264891136626495\n",
      "Epoch [65/300], Loss: 41.42322206277452\n",
      "Epoch [66/300], Loss: 41.22150466848628\n",
      "Epoch [67/300], Loss: 41.78297614172307\n",
      "Epoch [68/300], Loss: 42.14976578813544\n",
      "Epoch [69/300], Loss: 41.50038561842958\n",
      "Epoch [70/300], Loss: 40.22972662437896\n",
      "Epoch [71/300], Loss: 40.847352621192755\n",
      "Epoch [72/300], Loss: 41.06878008161272\n",
      "Epoch [73/300], Loss: 37.81291898384622\n",
      "Epoch [74/300], Loss: 40.44086403561078\n",
      "Epoch [75/300], Loss: 38.659957041938185\n",
      "Epoch [76/300], Loss: 37.27538081577846\n",
      "Epoch [77/300], Loss: 36.78470414684665\n",
      "Epoch [78/300], Loss: 40.94803025206113\n",
      "Epoch [79/300], Loss: 36.33428025135796\n",
      "Epoch [80/300], Loss: 39.00250560567126\n",
      "Epoch [81/300], Loss: 36.6201667258267\n",
      "Epoch [82/300], Loss: 36.48107164796047\n",
      "Epoch [83/300], Loss: 35.1289071289625\n",
      "Epoch [84/300], Loss: 36.091004543040754\n",
      "Epoch [85/300], Loss: 37.69080588345154\n",
      "Epoch [86/300], Loss: 36.424856792397215\n",
      "Epoch [87/300], Loss: 35.022263364308444\n",
      "Epoch [88/300], Loss: 36.4346681234474\n",
      "Epoch [89/300], Loss: 35.600521182134955\n",
      "Epoch [90/300], Loss: 35.41274806431362\n",
      "Epoch [91/300], Loss: 33.4344860551544\n",
      "Epoch [92/300], Loss: 34.12445483229677\n",
      "Epoch [93/300], Loss: 34.000453474334854\n",
      "Epoch [94/300], Loss: 32.66922798684116\n",
      "Epoch [95/300], Loss: 33.21371713102138\n",
      "Epoch [96/300], Loss: 34.29935662757416\n",
      "Epoch [97/300], Loss: 31.70441218784877\n",
      "Epoch [98/300], Loss: 33.27664121285012\n",
      "Epoch [99/300], Loss: 32.73620394517749\n",
      "Epoch [100/300], Loss: 35.05025594684935\n",
      "Epoch [101/300], Loss: 33.06238025014851\n",
      "Epoch [102/300], Loss: 30.742252929968767\n",
      "Epoch [103/300], Loss: 31.12514393889959\n",
      "Epoch [104/300], Loss: 31.308707522906467\n",
      "Epoch [105/300], Loss: 32.78095132854128\n",
      "Epoch [106/300], Loss: 37.278218071581584\n",
      "Epoch [107/300], Loss: 31.837411660752537\n",
      "Epoch [108/300], Loss: 30.363813690326182\n",
      "Epoch [109/300], Loss: 28.658639547462286\n",
      "Epoch [110/300], Loss: 27.382992827947238\n",
      "Epoch [111/300], Loss: 28.673583606421122\n",
      "Epoch [112/300], Loss: 28.50297068231117\n",
      "Epoch [113/300], Loss: 27.656749883555047\n",
      "Epoch [114/300], Loss: 29.19064144714637\n",
      "Epoch [115/300], Loss: 30.55163342172649\n",
      "Epoch [116/300], Loss: 29.5360354937716\n",
      "Epoch [117/300], Loss: 27.222122297858313\n",
      "Epoch [118/300], Loss: 26.913994855045722\n",
      "Epoch [119/300], Loss: 28.036405326034615\n",
      "Epoch [120/300], Loss: 28.840926561487436\n",
      "Epoch [121/300], Loss: 26.35051987467823\n",
      "Epoch [122/300], Loss: 26.212457296485724\n",
      "Epoch [123/300], Loss: 25.725369466614612\n",
      "Epoch [124/300], Loss: 25.682370269353488\n",
      "Epoch [125/300], Loss: 28.18817131640175\n",
      "Epoch [126/300], Loss: 28.615294038974746\n",
      "Epoch [127/300], Loss: 24.825497280068113\n",
      "Epoch [128/300], Loss: 27.043096867574526\n",
      "Epoch [129/300], Loss: 29.162549805531302\n",
      "Epoch [130/300], Loss: 27.93652864095802\n",
      "Epoch [131/300], Loss: 28.364131698959984\n",
      "Epoch [132/300], Loss: 27.376820682929957\n",
      "Epoch [133/300], Loss: 26.512557719709687\n",
      "Epoch [134/300], Loss: 25.699621763097525\n",
      "Epoch [135/300], Loss: 25.70780289118191\n",
      "Epoch [136/300], Loss: 25.98400151125297\n",
      "Epoch [137/300], Loss: 23.36240001977314\n",
      "Epoch [138/300], Loss: 24.268557649603636\n",
      "Epoch [139/300], Loss: 22.760777064732142\n",
      "Epoch [140/300], Loss: 25.407076330228886\n",
      "Epoch [141/300], Loss: 24.345389405703216\n",
      "Epoch [142/300], Loss: 23.517740170527162\n",
      "Epoch [143/300], Loss: 23.259319533950173\n",
      "Epoch [144/300], Loss: 22.732677459716797\n",
      "Epoch [145/300], Loss: 24.338824575397826\n",
      "Epoch [146/300], Loss: 23.828073281846287\n",
      "Epoch [147/300], Loss: 25.054141963132516\n",
      "Epoch [148/300], Loss: 22.119096958142823\n",
      "Epoch [149/300], Loss: 22.815196551485545\n",
      "Epoch [150/300], Loss: 24.169858976443244\n",
      "Epoch [151/300], Loss: 23.66656501172325\n",
      "Epoch [152/300], Loss: 24.32877508066766\n",
      "Epoch [153/300], Loss: 24.35050011560115\n",
      "Epoch [154/300], Loss: 22.92574496642785\n",
      "Epoch [155/300], Loss: 30.47191408922046\n",
      "Epoch [156/300], Loss: 29.076312614476077\n",
      "Epoch [157/300], Loss: 25.184892856580323\n",
      "Epoch [158/300], Loss: 23.395607381372407\n",
      "Epoch [159/300], Loss: 22.431736888973393\n",
      "Epoch [160/300], Loss: 22.16151892314858\n",
      "Epoch [161/300], Loss: 20.75359383156772\n",
      "Epoch [162/300], Loss: 20.87011912121751\n",
      "Epoch [163/300], Loss: 21.601975247607253\n",
      "Epoch [164/300], Loss: 20.673958650931784\n",
      "Epoch [165/300], Loss: 20.328778860206427\n",
      "Epoch [166/300], Loss: 22.447652860720588\n",
      "Epoch [167/300], Loss: 22.24865958218201\n",
      "Epoch [168/300], Loss: 20.32007324640652\n",
      "Epoch [169/300], Loss: 20.560179486252746\n",
      "Epoch [170/300], Loss: 19.897544895998344\n",
      "Epoch [171/300], Loss: 20.29871903696368\n",
      "Epoch [172/300], Loss: 19.656144524499567\n",
      "Epoch [173/300], Loss: 22.422164512669436\n",
      "Epoch [174/300], Loss: 19.10377564847744\n",
      "Epoch [175/300], Loss: 19.14993561248076\n",
      "Epoch [176/300], Loss: 18.995392513714627\n",
      "Epoch [177/300], Loss: 18.941606055756317\n",
      "Epoch [178/300], Loss: 18.867972765100717\n",
      "Epoch [179/300], Loss: 18.891653878348215\n",
      "Epoch [180/300], Loss: 19.67715264245662\n",
      "Epoch [181/300], Loss: 19.46243862732215\n",
      "Epoch [182/300], Loss: 19.437366626229704\n",
      "Epoch [183/300], Loss: 19.711308246384018\n",
      "Epoch [184/300], Loss: 19.982274701518396\n",
      "Epoch [185/300], Loss: 19.679979631977698\n",
      "Epoch [186/300], Loss: 22.53744341590987\n",
      "Epoch [187/300], Loss: 19.45599590248776\n",
      "Epoch [188/300], Loss: 23.603861408848918\n",
      "Epoch [189/300], Loss: 18.707198041924684\n",
      "Epoch [190/300], Loss: 20.130977999779486\n",
      "Epoch [191/300], Loss: 18.40293989752844\n",
      "Epoch [192/300], Loss: 18.02470443655269\n",
      "Epoch [193/300], Loss: 17.974444270683325\n",
      "Epoch [194/300], Loss: 17.085577600013277\n",
      "Epoch [195/300], Loss: 17.883138173186833\n",
      "Epoch [196/300], Loss: 19.077793490502142\n",
      "Epoch [197/300], Loss: 19.255557644751764\n",
      "Epoch [198/300], Loss: 17.76434860581077\n",
      "Epoch [199/300], Loss: 17.961809413224323\n",
      "Epoch [200/300], Loss: 17.605085733299433\n",
      "Epoch [201/300], Loss: 17.843700426514797\n",
      "Epoch [202/300], Loss: 16.389115012735815\n",
      "Epoch [203/300], Loss: 18.325276880220333\n",
      "Epoch [204/300], Loss: 17.347064550021827\n",
      "Epoch [205/300], Loss: 17.299128765334732\n",
      "Epoch [206/300], Loss: 19.518367152060232\n",
      "Epoch [207/300], Loss: 18.05602781234249\n",
      "Epoch [208/300], Loss: 17.15893320883474\n",
      "Epoch [209/300], Loss: 15.75348886586554\n",
      "Epoch [210/300], Loss: 16.205033051802815\n",
      "Epoch [211/300], Loss: 16.70471423452351\n",
      "Epoch [212/300], Loss: 16.881969504642047\n",
      "Epoch [213/300], Loss: 16.203358628233456\n",
      "Epoch [214/300], Loss: 15.105811734353342\n",
      "Epoch [215/300], Loss: 15.50207629401563\n",
      "Epoch [216/300], Loss: 14.60764608075542\n",
      "Epoch [217/300], Loss: 14.79028081234699\n",
      "Epoch [218/300], Loss: 15.342587343558737\n",
      "Epoch [219/300], Loss: 16.164062359366\n",
      "Epoch [220/300], Loss: 15.078605458483718\n",
      "Epoch [221/300], Loss: 15.78351563132853\n",
      "Epoch [222/300], Loss: 16.122629912767543\n",
      "Epoch [223/300], Loss: 15.840843130366594\n",
      "Epoch [224/300], Loss: 16.06545632006386\n",
      "Epoch [225/300], Loss: 14.309395135273032\n",
      "Epoch [226/300], Loss: 14.98319585422217\n",
      "Epoch [227/300], Loss: 15.54586152107485\n",
      "Epoch [228/300], Loss: 17.54911932527744\n",
      "Epoch [229/300], Loss: 14.10580342376287\n",
      "Epoch [230/300], Loss: 13.69372811734951\n",
      "Epoch [231/300], Loss: 14.506243833199076\n",
      "Epoch [232/300], Loss: 14.046100792247579\n",
      "Epoch [233/300], Loss: 16.3620476261262\n",
      "Epoch [234/300], Loss: 21.011294633012763\n",
      "Epoch [235/300], Loss: 15.992139051586802\n",
      "Epoch [236/300], Loss: 13.868621527324624\n",
      "Epoch [237/300], Loss: 13.5187747797109\n",
      "Epoch [238/300], Loss: 12.879740315098916\n",
      "Epoch [239/300], Loss: 13.824257898989911\n",
      "Epoch [240/300], Loss: 13.750679508332283\n",
      "Epoch [241/300], Loss: 13.04425774851153\n",
      "Epoch [242/300], Loss: 13.131200302581084\n",
      "Epoch [243/300], Loss: 14.421707891648815\n",
      "Epoch [244/300], Loss: 17.431438955843173\n",
      "Epoch [245/300], Loss: 14.768262537943054\n",
      "Epoch [246/300], Loss: 14.002547075122182\n",
      "Epoch [247/300], Loss: 13.609109377531412\n",
      "Epoch [248/300], Loss: 12.981182274181172\n",
      "Epoch [249/300], Loss: 12.939232962472099\n",
      "Epoch [250/300], Loss: 12.340116667857368\n",
      "Epoch [251/300], Loss: 12.343941583062097\n",
      "Epoch [252/300], Loss: 11.65854587642828\n",
      "Epoch [253/300], Loss: 12.222999660650157\n",
      "Epoch [254/300], Loss: 14.73887439147668\n",
      "Epoch [255/300], Loss: 14.208740937545002\n",
      "Epoch [256/300], Loss: 12.92735383697369\n",
      "Epoch [257/300], Loss: 13.599987135504797\n",
      "Epoch [258/300], Loss: 12.471791350896458\n",
      "Epoch [259/300], Loss: 12.890062253046695\n",
      "Epoch [260/300], Loss: 13.476405517296858\n",
      "Epoch [261/300], Loss: 12.263720094882947\n",
      "Epoch [262/300], Loss: 12.449232690345307\n",
      "Epoch [263/300], Loss: 11.851609638759069\n",
      "Epoch [264/300], Loss: 12.028029182539557\n",
      "Epoch [265/300], Loss: 11.337823771111976\n",
      "Epoch [266/300], Loss: 10.832387317710209\n",
      "Epoch [267/300], Loss: 12.193959513018209\n",
      "Epoch [268/300], Loss: 14.376852518951837\n",
      "Epoch [269/300], Loss: 14.083344314504878\n",
      "Epoch [270/300], Loss: 13.11646240867228\n",
      "Epoch [271/300], Loss: 11.567194943054481\n",
      "Epoch [272/300], Loss: 11.363599803590555\n",
      "Epoch [273/300], Loss: 12.814958036220569\n",
      "Epoch [274/300], Loss: 15.666446070517264\n",
      "Epoch [275/300], Loss: 13.090714520573068\n",
      "Epoch [276/300], Loss: 11.102134106895342\n",
      "Epoch [277/300], Loss: 10.774175002278271\n",
      "Epoch [278/300], Loss: 12.033524438532815\n",
      "Epoch [279/300], Loss: 15.601724123625162\n",
      "Epoch [280/300], Loss: 12.670340735791466\n",
      "Epoch [281/300], Loss: 10.935001628190141\n",
      "Epoch [282/300], Loss: 12.216934327156313\n",
      "Epoch [283/300], Loss: 10.72973896501251\n",
      "Epoch [284/300], Loss: 11.843399786180065\n",
      "Epoch [285/300], Loss: 11.840646488875288\n",
      "Epoch [286/300], Loss: 11.765759534000802\n",
      "Epoch [287/300], Loss: 12.83478130393314\n",
      "Epoch [288/300], Loss: 10.795137185654882\n",
      "Epoch [289/300], Loss: 10.678994420486662\n",
      "Epoch [290/300], Loss: 9.933329393237416\n",
      "Epoch [291/300], Loss: 9.89412082724857\n",
      "Epoch [292/300], Loss: 10.75225585726549\n",
      "Epoch [293/300], Loss: 14.757970133135396\n",
      "Epoch [294/300], Loss: 11.72788569015292\n",
      "Epoch [295/300], Loss: 12.210784384731873\n",
      "Epoch [296/300], Loss: 10.433737601003338\n",
      "Epoch [297/300], Loss: 11.40864751086257\n",
      "Epoch [298/300], Loss: 11.410727768999092\n",
      "Epoch [299/300], Loss: 10.863197396977156\n",
      "Epoch [300/300], Loss: 11.030582322503015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Clipping input data to the valid range for imshow with RGB data ([0..1] for floats or [0..255] for integers).\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# IMPORTANT: \n",
    "# training code works only for \n",
    "# used_embedding = \"naive\"\n",
    "# training and visualization code should work in both\n",
    "# used_embedding = \"learned\"\n",
    "# used_embedding = \"positonal\"\n",
    "used_embedding = \"positional\"\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "model = train(used_embedding=used_embedding)\n",
    "visualize_model_output(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "6fa2fa4f4d9d3d9ca73eb3739cc0e85a72773041ed8c7376d5dc2c41e6946bf8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

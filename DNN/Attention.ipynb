{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Your task is to modify the custom implementation of MultiHeadAttention. At present, every token in a sequence can attend to every other token.  \n",
        "\n",
        "\n",
        "Your job is to change this behavior in a specific way.\n",
        "Let $S$ be our input sequence of length $2 \\cdot k$:\n",
        "- tokens on positions $i \\le k$ should attend to prefix of $S$ of length $k$ ($S[:k]$)\n",
        "- tokens on positions $i \\gt k$ should attend to prefix of $S$  of length $i$ ($S[:i]$)\n",
        "\n",
        "(Note: You can assume the sequence length is always an even number)"
      ],
      "metadata": {
        "id": "VFiew9xpybfu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import math\n",
        "import torch.nn.functional as F\n",
        "class MultiHeadAttention(torch.nn.Module):\n",
        "    def __init__(self, d_model, num_heads, d_head):\n",
        "      super().__init__()\n",
        "      self.d_model = d_model\n",
        "      self.num_heads = num_heads\n",
        "      self.d_head = d_head\n",
        "\n",
        "      self.W_Q = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
        "      self.W_K = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
        "      self.W_V = torch.nn.Linear(d_model, num_heads*d_head, bias=True)\n",
        "      self.W_O = torch.nn.Linear(num_heads*d_head, d_model, bias=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      seq_len, batch_size, _ = x.shape\n",
        "\n",
        "      Q = self.W_Q(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
        "      K = self.W_K(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
        "      V = self.W_V(x).reshape(seq_len, batch_size, self.num_heads, self.d_head)\n",
        "\n",
        "      scaled_QK = torch.einsum(\"ibhd,jbhd->bhij\", Q, K) / math.sqrt(self.d_head)\n",
        "      # shape of scaled_QK is (batch_size, num_heads, seq_len, seq_len)\n",
        "      #TODO\n",
        "      mask_1 = torch.triu(torch.ones(seq_len, seq_len), 1).bool()\n",
        "      mask_2 = torch.ones(seq_len, seq_len, dtype=torch.bool)\n",
        "      k = int(seq_len/2)\n",
        "      mask_2[:k, :k] = False\n",
        "      mask = mask_1 & mask_2\n",
        "      scaled_QK.masked_fill_(mask, float('-inf'))\n",
        "      #END TODO\n",
        "      weights = F.softmax(scaled_QK, -1)\n",
        "      attention = torch.einsum(\"bhij,jbhd->ibhd\", weights, V)\n",
        "\n",
        "      result = self.W_O(attention.reshape(seq_len, batch_size,self.num_heads * self.d_head))\n",
        "\n",
        "      return result, weights"
      ],
      "metadata": {
        "id": "ENFiK_cTeDM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your solution\n",
        "d_model = 4\n",
        "num_heads= 4\n",
        "d_head = 2\n",
        "k = 3\n",
        "batch_size = 3\n",
        "\n",
        "with torch.no_grad():\n",
        "  mha = MultiHeadAttention(d_model, num_heads, d_head)\n",
        "  batched_x= torch.randn((2*k, batch_size, d_model))\n",
        "  result, weights = mha(batched_x)\n",
        "print(\"Result:\", result)\n",
        "print(\"Weights:\", weights)"
      ],
      "metadata": {
        "id": "GDQ0a57NeB-z",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cdf5847-aa0e-4c9c-8121-726ae317e595"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Result: tensor([[[-1.2155e-01,  2.6393e-01,  2.8529e-01,  8.7492e-03],\n",
            "         [-3.0860e-01,  7.6529e-02,  7.5335e-01, -9.3537e-02],\n",
            "         [-3.2243e-01,  3.0948e-01,  4.6269e-01,  1.4608e-03]],\n",
            "\n",
            "        [[-9.7908e-02,  2.5062e-01,  3.2149e-01,  1.7905e-03],\n",
            "         [-5.0542e-01,  1.9742e-01,  5.0122e-01, -2.4212e-03],\n",
            "         [-2.9994e-01,  3.1374e-01,  4.0071e-01,  1.3043e-02]],\n",
            "\n",
            "        [[-8.6501e-02,  2.4332e-01,  3.4093e-01, -4.7499e-03],\n",
            "         [-5.8198e-01, -5.3442e-03,  8.7805e-01, -1.1195e-01],\n",
            "         [-3.1591e-01,  2.6517e-01,  4.2930e-01,  3.7990e-03]],\n",
            "\n",
            "        [[-6.2012e-02,  2.6314e-01,  3.1483e-01,  4.7588e-04],\n",
            "         [-4.3939e-01,  1.0091e-01,  6.7119e-01, -6.6481e-02],\n",
            "         [ 5.2178e-02,  3.4111e-01,  5.5215e-02,  2.5473e-02]],\n",
            "\n",
            "        [[-1.3085e-01,  3.0381e-01,  2.6836e-01,  2.4589e-02],\n",
            "         [-5.6368e-01,  7.1615e-02,  7.3330e-01, -6.9056e-02],\n",
            "         [ 1.3761e-02,  3.3741e-01,  2.3844e-01,  1.4618e-02]],\n",
            "\n",
            "        [[-1.2987e-01,  3.1723e-01,  2.7586e-01,  2.5461e-02],\n",
            "         [-5.2862e-01,  1.8928e-01,  6.4525e-01, -3.7548e-02],\n",
            "         [-5.9765e-03,  1.9588e-01,  2.1517e-02,  3.6084e-03]]])\n",
            "Weights: tensor([[[[0.4062, 0.3193, 0.2745, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2933, 0.3136, 0.3931, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2463, 0.2950, 0.4587, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1787, 0.2270, 0.3478, 0.2464, 0.0000, 0.0000],\n",
            "          [0.2588, 0.1936, 0.1562, 0.1690, 0.2224, 0.0000],\n",
            "          [0.2061, 0.1646, 0.1431, 0.1476, 0.1795, 0.1592]],\n",
            "\n",
            "         [[0.3660, 0.3649, 0.2691, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3229, 0.3332, 0.3439, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3425, 0.3365, 0.3210, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2243, 0.2375, 0.3004, 0.2377, 0.0000, 0.0000],\n",
            "          [0.1843, 0.1896, 0.1670, 0.1921, 0.2671, 0.0000],\n",
            "          [0.1420, 0.1498, 0.1570, 0.1513, 0.1872, 0.2127]],\n",
            "\n",
            "         [[0.3980, 0.4044, 0.1977, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3620, 0.3636, 0.2744, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3433, 0.3437, 0.3130, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2535, 0.2529, 0.2407, 0.2529, 0.0000, 0.0000],\n",
            "          [0.1875, 0.1911, 0.0984, 0.2083, 0.3147, 0.0000],\n",
            "          [0.1435, 0.1461, 0.0862, 0.1570, 0.2086, 0.2586]],\n",
            "\n",
            "         [[0.5818, 0.2256, 0.1925, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3761, 0.2841, 0.3397, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3335, 0.3180, 0.3485, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2072, 0.2283, 0.3320, 0.2326, 0.0000, 0.0000],\n",
            "          [0.3785, 0.1275, 0.1466, 0.0662, 0.2812, 0.0000],\n",
            "          [0.3227, 0.1234, 0.1760, 0.0676, 0.1941, 0.1162]]],\n",
            "\n",
            "\n",
            "        [[[0.2623, 0.1521, 0.5856, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3560, 0.3872, 0.2568, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3507, 0.1455, 0.5038, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2534, 0.1541, 0.3684, 0.2240, 0.0000, 0.0000],\n",
            "          [0.2025, 0.1311, 0.2602, 0.1765, 0.2296, 0.0000],\n",
            "          [0.1966, 0.1808, 0.1286, 0.1593, 0.2036, 0.1311]],\n",
            "\n",
            "         [[0.1582, 0.1829, 0.6589, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2274, 0.2883, 0.4843, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4484, 0.3795, 0.1721, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2098, 0.2259, 0.3207, 0.2435, 0.0000, 0.0000],\n",
            "          [0.2111, 0.2267, 0.1837, 0.1982, 0.1803, 0.0000],\n",
            "          [0.1213, 0.1328, 0.2426, 0.1555, 0.1381, 0.2096]],\n",
            "\n",
            "         [[0.2941, 0.3144, 0.3915, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2391, 0.4956, 0.2653, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3532, 0.3140, 0.3328, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2250, 0.2783, 0.2519, 0.2449, 0.0000, 0.0000],\n",
            "          [0.1818, 0.2483, 0.2176, 0.2055, 0.1468, 0.0000],\n",
            "          [0.1383, 0.2107, 0.1267, 0.1688, 0.1056, 0.2499]],\n",
            "\n",
            "         [[0.6344, 0.1594, 0.2062, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1666, 0.4180, 0.4154, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3962, 0.2871, 0.3167, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2936, 0.2234, 0.2433, 0.2398, 0.0000, 0.0000],\n",
            "          [0.1864, 0.2035, 0.2055, 0.1975, 0.2072, 0.0000],\n",
            "          [0.1108, 0.1578, 0.1952, 0.1355, 0.3145, 0.0862]]],\n",
            "\n",
            "\n",
            "        [[[0.3717, 0.3380, 0.2902, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4001, 0.2861, 0.3138, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3867, 0.3355, 0.2778, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3488, 0.2676, 0.2307, 0.1529, 0.0000, 0.0000],\n",
            "          [0.2758, 0.1850, 0.1813, 0.1092, 0.2487, 0.0000],\n",
            "          [0.1945, 0.1544, 0.1688, 0.1332, 0.1687, 0.1804]],\n",
            "\n",
            "         [[0.2530, 0.4675, 0.2795, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2856, 0.3999, 0.3145, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2890, 0.4454, 0.2656, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2417, 0.3035, 0.2194, 0.2354, 0.0000, 0.0000],\n",
            "          [0.2081, 0.1914, 0.2081, 0.1989, 0.1934, 0.0000],\n",
            "          [0.2184, 0.3106, 0.1196, 0.1038, 0.1570, 0.0905]],\n",
            "\n",
            "         [[0.3151, 0.4460, 0.2389, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2770, 0.3666, 0.3564, 0.0000, 0.0000, 0.0000],\n",
            "          [0.2955, 0.4557, 0.2488, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1985, 0.2769, 0.2240, 0.3005, 0.0000, 0.0000],\n",
            "          [0.1759, 0.2117, 0.1956, 0.2364, 0.1805, 0.0000],\n",
            "          [0.1106, 0.3234, 0.1173, 0.2410, 0.1219, 0.0858]],\n",
            "\n",
            "         [[0.5480, 0.1587, 0.2933, 0.0000, 0.0000, 0.0000],\n",
            "          [0.4179, 0.2292, 0.3529, 0.0000, 0.0000, 0.0000],\n",
            "          [0.3636, 0.3102, 0.3262, 0.0000, 0.0000, 0.0000],\n",
            "          [0.1700, 0.2760, 0.2152, 0.3388, 0.0000, 0.0000],\n",
            "          [0.2161, 0.1778, 0.2074, 0.1665, 0.2323, 0.0000],\n",
            "          [0.0362, 0.1978, 0.0954, 0.4242, 0.1064, 0.1399]]]])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "uscngnJ95IPj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}